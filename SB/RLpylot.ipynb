{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from boat_env import BoatEnv\n",
    "\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines import DQN\n",
    "\n",
    "from stable_baselines.common.vec_env import VecVideoRecorder, DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "boatenv = BoatEnv(type='discrete', mode='simulation')\n",
    "check_env(boatenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deactivate all the DQN extensions to have the original version\n",
    "# In practice, it is recommend to have them activated\n",
    "# kwargs = {'double_q': False, 'prioritized_replay': False, 'policy_kwargs': dict(dueling=False)}\n",
    "\n",
    "# Note that the MlpPolicy of DQN is different from the one of PPO\n",
    "# but stable-baselines handles that automatically if you pass a string\n",
    "dqn_model = DQN('MlpPolicy', boatenv, verbose=1, tensorboard_log=\"./log/\") #, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Benj\\Anaconda3\\envs\\CityLearn\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:322: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 50       |\n",
      "| episodes                | 10       |\n",
      "| mean 100 episode reward | 0.6      |\n",
      "| steps                   | 2528     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 20       |\n",
      "| mean 100 episode reward | 1.7      |\n",
      "| steps                   | 6411     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 10559    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.deepq.dqn.DQN at 0x17579c9f988>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the agent for x steps\n",
    "dqn_model.learn(total_timesteps=50000, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = boatenv.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    action, _states = dqn_model.predict(obs)\n",
    "    obs, rewards, dones, info = boatenv.step(action)\n",
    "    boatenv.render()\n",
    "    if dones == True:\n",
    "        obs = boatenv.reset()\n",
    "    \n",
    "boatenv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.sac.policies import MlpPolicy\n",
    "from stable_baselines import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boatenv = BoatEnv(type='continuous', mode='simulation')\n",
    "check_env(boatenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Benj\\Anaconda3\\envs\\CityLearn\\lib\\site-packages\\stable_baselines\\common\\base_class.py:1143: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Benj\\Anaconda3\\envs\\CityLearn\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:502: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "-------------------------------------------\n",
      "| current_lr              | 0.0003        |\n",
      "| ent_coef                | 0.98896474    |\n",
      "| ent_coef_loss           | -0.018586244  |\n",
      "| entropy                 | 1.2762728     |\n",
      "| episodes                | 2             |\n",
      "| fps                     | 214           |\n",
      "| mean 100 episode reward | -1            |\n",
      "| n_updates               | 38            |\n",
      "| policy_loss             | -0.63328016   |\n",
      "| qf1_loss                | 0.00013312467 |\n",
      "| qf2_loss                | 1.813969e-05  |\n",
      "| time_elapsed            | 0             |\n",
      "| total timesteps         | 137           |\n",
      "| value_loss              | 0.08337617    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| current_lr              | 0.0003        |\n",
      "| ent_coef                | 0.8524365     |\n",
      "| ent_coef_loss           | -0.2687397    |\n",
      "| entropy                 | 1.289706      |\n",
      "| episodes                | 3             |\n",
      "| fps                     | 142           |\n",
      "| mean 100 episode reward | 1.9           |\n",
      "| n_updates               | 533           |\n",
      "| policy_loss             | -1.5719702    |\n",
      "| qf1_loss                | 0.0002924968  |\n",
      "| qf2_loss                | 0.00028219927 |\n",
      "| time_elapsed            | 4             |\n",
      "| total timesteps         | 632           |\n",
      "| value_loss              | 0.0032724456  |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.6592442    |\n",
      "| ent_coef_loss           | -0.7141934   |\n",
      "| entropy                 | 1.2801156    |\n",
      "| episodes                | 4            |\n",
      "| fps                     | 123          |\n",
      "| mean 100 episode reward | 0.9          |\n",
      "| n_updates               | 1390         |\n",
      "| policy_loss             | -3.2616162   |\n",
      "| qf1_loss                | 0.10782558   |\n",
      "| qf2_loss                | 0.11064012   |\n",
      "| time_elapsed            | 12           |\n",
      "| total timesteps         | 1489         |\n",
      "| value_loss              | 0.0049961307 |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.64746463   |\n",
      "| ent_coef_loss           | -0.71962285  |\n",
      "| entropy                 | 1.2816265    |\n",
      "| episodes                | 5            |\n",
      "| fps                     | 120          |\n",
      "| mean 100 episode reward | 0.4          |\n",
      "| n_updates               | 1450         |\n",
      "| policy_loss             | -3.3150759   |\n",
      "| qf1_loss                | 0.008054737  |\n",
      "| qf2_loss                | 0.007893187  |\n",
      "| time_elapsed            | 12           |\n",
      "| total timesteps         | 1549         |\n",
      "| value_loss              | 0.0026312391 |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.61895394   |\n",
      "| ent_coef_loss           | -0.80776393  |\n",
      "| entropy                 | 1.2938683    |\n",
      "| episodes                | 6            |\n",
      "| fps                     | 120          |\n",
      "| mean 100 episode reward | 0.2          |\n",
      "| n_updates               | 1600         |\n",
      "| policy_loss             | -3.6176674   |\n",
      "| qf1_loss                | 0.001434156  |\n",
      "| qf2_loss                | 0.0010287402 |\n",
      "| time_elapsed            | 14           |\n",
      "| total timesteps         | 1699         |\n",
      "| value_loss              | 0.0029721973 |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| current_lr              | 0.0003        |\n",
      "| ent_coef                | 0.55074424    |\n",
      "| ent_coef_loss           | -0.9983089    |\n",
      "| entropy                 | 1.3036699     |\n",
      "| episodes                | 7             |\n",
      "| fps                     | 128           |\n",
      "| mean 100 episode reward | -0            |\n",
      "| n_updates               | 1989          |\n",
      "| policy_loss             | -4.2504025    |\n",
      "| qf1_loss                | 0.000770383   |\n",
      "| qf2_loss                | 0.00053641415 |\n",
      "| time_elapsed            | 16            |\n",
      "| total timesteps         | 2088          |\n",
      "| value_loss              | 0.0034611044  |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.473193    |\n",
      "| ent_coef_loss           | -1.2637105  |\n",
      "| entropy                 | 1.2955817   |\n",
      "| episodes                | 8           |\n",
      "| fps                     | 128         |\n",
      "| mean 100 episode reward | 0.7         |\n",
      "| n_updates               | 2495        |\n",
      "| policy_loss             | -4.9228964  |\n",
      "| qf1_loss                | 0.021968588 |\n",
      "| qf2_loss                | 0.022069907 |\n",
      "| time_elapsed            | 20          |\n",
      "| total timesteps         | 2594        |\n",
      "| value_loss              | 0.004336344 |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.4444515    |\n",
      "| ent_coef_loss           | -1.3551846   |\n",
      "| entropy                 | 1.2846967    |\n",
      "| episodes                | 9            |\n",
      "| fps                     | 130          |\n",
      "| mean 100 episode reward | 0.5          |\n",
      "| n_updates               | 2704         |\n",
      "| policy_loss             | -5.1348877   |\n",
      "| qf1_loss                | 0.0017680028 |\n",
      "| qf2_loss                | 0.0014811836 |\n",
      "| time_elapsed            | 21           |\n",
      "| total timesteps         | 2803         |\n",
      "| value_loss              | 0.0047553713 |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.41033024   |\n",
      "| ent_coef_loss           | -1.48636     |\n",
      "| entropy                 | 1.3141487    |\n",
      "| episodes                | 10           |\n",
      "| fps                     | 132          |\n",
      "| mean 100 episode reward | 0.3          |\n",
      "| n_updates               | 2970         |\n",
      "| policy_loss             | -5.403281    |\n",
      "| qf1_loss                | 0.0016082369 |\n",
      "| qf2_loss                | 0.0018322247 |\n",
      "| time_elapsed            | 23           |\n",
      "| total timesteps         | 3069         |\n",
      "| value_loss              | 0.003974883  |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.39466354   |\n",
      "| ent_coef_loss           | -1.5778649   |\n",
      "| entropy                 | 1.2810163    |\n",
      "| episodes                | 11           |\n",
      "| fps                     | 134          |\n",
      "| mean 100 episode reward | 0.2          |\n",
      "| n_updates               | 3100         |\n",
      "| policy_loss             | -5.6011076   |\n",
      "| qf1_loss                | 0.006927764  |\n",
      "| qf2_loss                | 0.0072927903 |\n",
      "| time_elapsed            | 23           |\n",
      "| total timesteps         | 3199         |\n",
      "| value_loss              | 0.003011316  |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.3278829  |\n",
      "| ent_coef_loss           | -1.9106255 |\n",
      "| entropy                 | 1.3188819  |\n",
      "| episodes                | 12         |\n",
      "| fps                     | 138        |\n",
      "| mean 100 episode reward | 0.9        |\n",
      "| n_updates               | 3718       |\n",
      "| policy_loss             | -5.974536  |\n",
      "| qf1_loss                | 0.2753972  |\n",
      "| qf2_loss                | 0.27427387 |\n",
      "| time_elapsed            | 27         |\n",
      "| total timesteps         | 3817       |\n",
      "| value_loss              | 0.00356414 |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.2849424    |\n",
      "| ent_coef_loss           | -2.096273    |\n",
      "| entropy                 | 1.2978499    |\n",
      "| episodes                | 13           |\n",
      "| fps                     | 142          |\n",
      "| mean 100 episode reward | 0.8          |\n",
      "| n_updates               | 4186         |\n",
      "| policy_loss             | -6.1869173   |\n",
      "| qf1_loss                | 0.0030326243 |\n",
      "| qf2_loss                | 0.003151487  |\n",
      "| time_elapsed            | 30           |\n",
      "| total timesteps         | 4285         |\n",
      "| value_loss              | 0.00427006   |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.22932354   |\n",
      "| ent_coef_loss           | -2.4820294   |\n",
      "| entropy                 | 1.3046303    |\n",
      "| episodes                | 14           |\n",
      "| fps                     | 142          |\n",
      "| mean 100 episode reward | 1.8          |\n",
      "| n_updates               | 4910         |\n",
      "| policy_loss             | -6.571297    |\n",
      "| qf1_loss                | 0.00822128   |\n",
      "| qf2_loss                | 0.0089412425 |\n",
      "| time_elapsed            | 35           |\n",
      "| total timesteps         | 5009         |\n",
      "| value_loss              | 0.0028117849 |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.14809883   |\n",
      "| ent_coef_loss           | -3.1595483   |\n",
      "| entropy                 | 1.3285587    |\n",
      "| episodes                | 15           |\n",
      "| fps                     | 147          |\n",
      "| mean 100 episode reward | 2.1          |\n",
      "| n_updates               | 6368         |\n",
      "| policy_loss             | -6.7107434   |\n",
      "| qf1_loss                | 0.008264837  |\n",
      "| qf2_loss                | 0.008049551  |\n",
      "| time_elapsed            | 43           |\n",
      "| total timesteps         | 6467         |\n",
      "| value_loss              | 0.0027824827 |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.12189469   |\n",
      "| ent_coef_loss           | -3.5730937   |\n",
      "| entropy                 | 1.3528355    |\n",
      "| episodes                | 16           |\n",
      "| fps                     | 146          |\n",
      "| mean 100 episode reward | 1.9          |\n",
      "| n_updates               | 7018         |\n",
      "| policy_loss             | -6.71326     |\n",
      "| qf1_loss                | 0.0010615225 |\n",
      "| qf2_loss                | 0.0010072618 |\n",
      "| time_elapsed            | 48           |\n",
      "| total timesteps         | 7117         |\n",
      "| value_loss              | 0.003530718  |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.11208679   |\n",
      "| ent_coef_loss           | -3.6656985   |\n",
      "| entropy                 | 1.2832942    |\n",
      "| episodes                | 17           |\n",
      "| fps                     | 143          |\n",
      "| mean 100 episode reward | 2            |\n",
      "| n_updates               | 7298         |\n",
      "| policy_loss             | -6.6259584   |\n",
      "| qf1_loss                | 0.0023578424 |\n",
      "| qf2_loss                | 0.002402977  |\n",
      "| time_elapsed            | 51           |\n",
      "| total timesteps         | 7397         |\n",
      "| value_loss              | 0.0016733032 |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| current_lr              | 0.0003        |\n",
      "| ent_coef                | 0.058728855   |\n",
      "| ent_coef_loss           | -4.756999     |\n",
      "| entropy                 | 1.2655311     |\n",
      "| episodes                | 18            |\n",
      "| fps                     | 148           |\n",
      "| mean 100 episode reward | 2.6           |\n",
      "| n_updates               | 9458          |\n",
      "| policy_loss             | -6.444191     |\n",
      "| qf1_loss                | 0.00060282863 |\n",
      "| qf2_loss                | 0.000536642   |\n",
      "| time_elapsed            | 64            |\n",
      "| total timesteps         | 9557          |\n",
      "| value_loss              | 0.0021852492  |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.0563672    |\n",
      "| ent_coef_loss           | -4.662669    |\n",
      "| entropy                 | 1.3102678    |\n",
      "| episodes                | 19           |\n",
      "| fps                     | 148          |\n",
      "| mean 100 episode reward | 2.4          |\n",
      "| n_updates               | 9595         |\n",
      "| policy_loss             | -6.5155125   |\n",
      "| qf1_loss                | 0.0014646356 |\n",
      "| qf2_loss                | 0.0010934048 |\n",
      "| time_elapsed            | 65           |\n",
      "| total timesteps         | 9694         |\n",
      "| value_loss              | 0.00131619   |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.05100863   |\n",
      "| ent_coef_loss           | -5.146427    |\n",
      "| entropy                 | 1.298326     |\n",
      "| episodes                | 20           |\n",
      "| fps                     | 147          |\n",
      "| mean 100 episode reward | 2.5          |\n",
      "| n_updates               | 9929         |\n",
      "| policy_loss             | -6.2262244   |\n",
      "| qf1_loss                | 0.006656928  |\n",
      "| qf2_loss                | 0.006421092  |\n",
      "| time_elapsed            | 67           |\n",
      "| total timesteps         | 10028        |\n",
      "| value_loss              | 0.0010535874 |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.047627173  |\n",
      "| ent_coef_loss           | -5.2527294   |\n",
      "| entropy                 | 1.3367298    |\n",
      "| episodes                | 21           |\n",
      "| fps                     | 147          |\n",
      "| mean 100 episode reward | 2.5          |\n",
      "| n_updates               | 10159        |\n",
      "| policy_loss             | -6.219216    |\n",
      "| qf1_loss                | 0.0018165072 |\n",
      "| qf2_loss                | 0.0018989264 |\n",
      "| time_elapsed            | 69           |\n",
      "| total timesteps         | 10258        |\n",
      "| value_loss              | 0.0049413573 |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.04295391  |\n",
      "| ent_coef_loss           | -5.116944   |\n",
      "| entropy                 | 1.2830874   |\n",
      "| episodes                | 22          |\n",
      "| fps                     | 148         |\n",
      "| mean 100 episode reward | 2.3         |\n",
      "| n_updates               | 10507       |\n",
      "| policy_loss             | -6.0082493  |\n",
      "| qf1_loss                | 0.008938345 |\n",
      "| qf2_loss                | 0.0091069   |\n",
      "| time_elapsed            | 71          |\n",
      "| total timesteps         | 10606       |\n",
      "| value_loss              | 0.003000246 |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| current_lr              | 0.0003        |\n",
      "| ent_coef                | 0.013889637   |\n",
      "| ent_coef_loss           | -6.800041     |\n",
      "| entropy                 | 1.2380166     |\n",
      "| episodes                | 23            |\n",
      "| fps                     | 147           |\n",
      "| mean 100 episode reward | 3.1           |\n",
      "| n_updates               | 14324         |\n",
      "| policy_loss             | -5.17034      |\n",
      "| qf1_loss                | 0.00047006493 |\n",
      "| qf2_loss                | 0.00034302007 |\n",
      "| time_elapsed            | 97            |\n",
      "| total timesteps         | 14423         |\n",
      "| value_loss              | 0.00081118016 |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| current_lr              | 0.0003        |\n",
      "| ent_coef                | 0.0042244205  |\n",
      "| ent_coef_loss           | -1.801772     |\n",
      "| entropy                 | 0.0046974756  |\n",
      "| episodes                | 24            |\n",
      "| fps                     | 151           |\n",
      "| mean 100 episode reward | 6.5           |\n",
      "| n_updates               | 19968         |\n",
      "| policy_loss             | -4.01794      |\n",
      "| qf1_loss                | 0.014037721   |\n",
      "| qf2_loss                | 0.014734633   |\n",
      "| time_elapsed            | 132           |\n",
      "| total timesteps         | 20067         |\n",
      "| value_loss              | 0.00085015397 |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| current_lr              | 0.0003        |\n",
      "| ent_coef                | 0.002823346   |\n",
      "| ent_coef_loss           | -2.564638     |\n",
      "| entropy                 | 0.3248591     |\n",
      "| episodes                | 25            |\n",
      "| fps                     | 149           |\n",
      "| mean 100 episode reward | 7.2           |\n",
      "| n_updates               | 22113         |\n",
      "| policy_loss             | -3.7594256    |\n",
      "| qf1_loss                | 0.00062761834 |\n",
      "| qf2_loss                | 0.0006525202  |\n",
      "| time_elapsed            | 148           |\n",
      "| total timesteps         | 22212         |\n",
      "| value_loss              | 0.0031000494  |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| current_lr              | 0.0003        |\n",
      "| ent_coef                | 0.0019941614  |\n",
      "| ent_coef_loss           | -0.83402056   |\n",
      "| entropy                 | 0.5808845     |\n",
      "| episodes                | 26            |\n",
      "| fps                     | 148           |\n",
      "| mean 100 episode reward | 7.5           |\n",
      "| n_updates               | 23428         |\n",
      "| policy_loss             | -3.4575891    |\n",
      "| qf1_loss                | 0.00027636273 |\n",
      "| qf2_loss                | 0.0003316409  |\n",
      "| time_elapsed            | 158           |\n",
      "| total timesteps         | 23527         |\n",
      "| value_loss              | 0.0014239855  |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| current_lr              | 0.0003        |\n",
      "| ent_coef                | 0.0017879144  |\n",
      "| ent_coef_loss           | -1.5397105    |\n",
      "| entropy                 | 0.52851164    |\n",
      "| episodes                | 27            |\n",
      "| fps                     | 147           |\n",
      "| mean 100 episode reward | 7.5           |\n",
      "| n_updates               | 24505         |\n",
      "| policy_loss             | -3.1588244    |\n",
      "| qf1_loss                | 0.0075550578  |\n",
      "| qf2_loss                | 0.007108152   |\n",
      "| time_elapsed            | 166           |\n",
      "| total timesteps         | 24604         |\n",
      "| value_loss              | 0.00096963963 |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| current_lr              | 0.0003        |\n",
      "| ent_coef                | 0.0017570222  |\n",
      "| ent_coef_loss           | -1.172138     |\n",
      "| entropy                 | 0.27833867    |\n",
      "| episodes                | 28            |\n",
      "| fps                     | 147           |\n",
      "| mean 100 episode reward | 7.3           |\n",
      "| n_updates               | 24677         |\n",
      "| policy_loss             | -3.2593107    |\n",
      "| qf1_loss                | 0.00043063902 |\n",
      "| qf2_loss                | 0.00062329776 |\n",
      "| time_elapsed            | 167           |\n",
      "| total timesteps         | 24776         |\n",
      "| value_loss              | 0.00031584327 |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.sac.sac.SAC at 0x2143a375188>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sac_model = SAC(MlpPolicy, boatenv, verbose=1, tensorboard_log=\"./log/\")\n",
    "sac_model.learn(total_timesteps=50000, log_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = boatenv.reset()\n",
    "\n",
    "for n in range(10):\n",
    "    for i in range(300):\n",
    "        action, _states = sac_model.predict(obs)\n",
    "        obs, rewards, dones, info = boatenv.step(action)\n",
    "        boatenv.render()\n",
    "        if dones == True:\n",
    "            obs = boatenv.reset()\n",
    "    obs = boatenv.reset()\n",
    "    \n",
    "boatenv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
